{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The Plastic Tide: Forecasting Macroplastic Pollution in Major River Systems\n",
        "\n",
        "This notebook implements the full pipeline to predict riverine plastic waste outflow from socioeconomic, geographic, and infrastructural features.\n",
        "\n",
        "- Data sources: Plastic River Footprints, World Bank, GRID3.\n",
        "- Models: Ridge, Lasso, Gradient Boosting (XGBoost).\n",
        "- Metrics: R-squared (RÂ²), MAPE.\n",
        "\n",
        "Run cells top to bottom. See `report/` for LaTeX report template."
      ],
      "id": "2ed69fe0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import json\n",
        "import math\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.metrics import r2_score, mean_absolute_percentage_error\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "PROJECT_ROOT = Path('..').resolve()\n",
        "DATA_DIR = PROJECT_ROOT / 'data'\n",
        "OUTPUTS_DIR = PROJECT_ROOT / 'outputs'\n",
        "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print('Project root:', PROJECT_ROOT)\n",
        "print('Data dir:', DATA_DIR)\n",
        "print('Outputs dir:', OUTPUTS_DIR)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "7abe61f2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data loading\n",
        "\n",
        "Provide CSV files in `data/` directory:\n",
        "- `rivers_plastic.csv`: target `plastic_kg_per_year` and river identifiers\n",
        "- `world_bank.csv`: socioeconomic indicators per country\n",
        "- `grid3_population.csv`: population density or totals near river buffers\n",
        "\n",
        "You may adapt filenames/columns below to your specific datasets."
      ],
      "id": "a67f1a4c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# File paths (adjust as needed)\n",
        "rivers_fp = DATA_DIR / 'rivers_plastic.csv'\n",
        "world_bank_fp = DATA_DIR / 'world_bank.csv'\n",
        "grid3_fp = DATA_DIR / 'grid3_population.csv'\n",
        "\n",
        "# Load with safe defaults\n",
        "rivers_df = pd.read_csv(rivers_fp) if rivers_fp.exists() else pd.DataFrame()\n",
        "wb_df = pd.read_csv(world_bank_fp) if world_bank_fp.exists() else pd.DataFrame()\n",
        "grid_df = pd.read_csv(grid3_fp) if grid3_fp.exists() else pd.DataFrame()\n",
        "\n",
        "print('rivers_df shape:', rivers_df.shape)\n",
        "print('wb_df shape:', wb_df.shape)\n",
        "print('grid_df shape:', grid_df.shape)\n",
        "\n",
        "# Basic sanity checks\n",
        "expected_target = 'plastic_kg_per_year'\n",
        "if not rivers_df.empty and expected_target not in rivers_df.columns:\n",
        "    raise ValueError(f\"Expected target column '{expected_target}' missing in rivers_plastic.csv\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "5b881b62"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data merging and feature engineering\n",
        "\n",
        "We merge river-level target data with country-level indicators and local population features. Replace keys as appropriate for your data (e.g., `country`, `river_id`)."
      ],
      "id": "bf92e572"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def safe_lower(series):\n",
        "    return series.astype(str).str.lower().str.strip()\n",
        "\n",
        "if not rivers_df.empty:\n",
        "    # Standardize join keys\n",
        "    if 'country' in rivers_df.columns:\n",
        "        rivers_df['country_key'] = safe_lower(rivers_df['country'])\n",
        "    if 'river_id' not in rivers_df.columns:\n",
        "        rivers_df['river_id'] = np.arange(len(rivers_df))\n",
        "\n",
        "if not wb_df.empty:\n",
        "    # Expect a country column\n",
        "    key_col = 'country' if 'country' in wb_df.columns else ('Country' if 'Country' in wb_df.columns else None)\n",
        "    if key_col:\n",
        "        wb_df['country_key'] = safe_lower(wb_df[key_col])\n",
        "\n",
        "if not grid_df.empty:\n",
        "    # Expect river_id or a join key present in rivers\n",
        "    if 'river_id' not in grid_df.columns and 'river_name' in grid_df.columns and 'river_name' in rivers_df.columns:\n",
        "        grid_df = grid_df.merge(\n",
        "            rivers_df[['river_id', 'river_name']].assign(river_name_key=safe_lower(rivers_df['river_name'])),\n",
        "            left_on=grid_df['river_name'].str.lower().str.strip(),\n",
        "            right_on='river_name_key',\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "# Merge\n",
        "df = rivers_df.copy()\n",
        "if not wb_df.empty and 'country_key' in df.columns and 'country_key' in wb_df.columns:\n",
        "    wb_cols = [c for c in wb_df.columns if c not in ['country', 'Country']]\n",
        "    df = df.merge(wb_df[wb_cols], on='country_key', how='left')\n",
        "\n",
        "if not grid_df.empty:\n",
        "    join_cols = ['river_id'] if 'river_id' in grid_df.columns else []\n",
        "    if join_cols:\n",
        "        df = df.merge(grid_df, on=join_cols, how='left')\n",
        "\n",
        "print('Merged df shape:', df.shape)\n",
        "print('Columns:', list(df.columns)[:20], '...')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "b82c1bc6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train/validation/test split and preprocessing\n",
        "\n",
        "We select numeric features, impute missing values, and scale for linear models. Tree models handle scaling implicitly."
      ],
      "id": "96e83f21"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Select target and features\n",
        "TARGET_COL = 'plastic_kg_per_year'\n",
        "if df.empty or TARGET_COL not in df.columns:\n",
        "    print('Dataset not ready. Please place CSVs in data/ and rerun.')\n",
        "else:\n",
        "    y = df[TARGET_COL].astype(float)\n",
        "    # Simple numeric-only baseline, drop identifiers and text columns\n",
        "    drop_like = {'river_name', 'country', 'country_key'}\n",
        "    numeric_cols = [c for c in df.columns if c not in drop_like and c != TARGET_COL and pd.api.types.is_numeric_dtype(df[c])]\n",
        "    X = df[numeric_cols].copy()\n",
        "\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "    print('Train/Valid/Test:', X_train.shape, X_valid.shape, X_test.shape)\n",
        "\n",
        "    # Preprocessing for linear models\n",
        "    numeric_pipeline = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[('num', numeric_pipeline, numeric_cols)],\n",
        "        remainder='drop'\n",
        "    )\n",
        "\n",
        "    # Baseline Ridge\n",
        "    ridge = Pipeline(steps=[('prep', preprocessor), ('model', Ridge())])\n",
        "    ridge_param_grid = {'model__alpha': [0.1, 1.0, 10.0, 100.0]}\n",
        "\n",
        "    ridge_cv = GridSearchCV(ridge, ridge_param_grid, scoring='r2', cv=5, n_jobs=-1)\n",
        "    ridge_cv.fit(X_train, y_train)\n",
        "    y_pred = ridge_cv.predict(X_valid)\n",
        "    print('Ridge best alpha:', ridge_cv.best_params_['model__alpha'])\n",
        "    print('Ridge valid R2:', r2_score(y_valid, y_pred))\n",
        "\n",
        "    # Lasso\n",
        "    lasso = Pipeline(steps=[('prep', preprocessor), ('model', Lasso(max_iter=5000))])\n",
        "    lasso_param_grid = {'model__alpha': [0.001, 0.01, 0.1, 1.0]}\n",
        "    lasso_cv = GridSearchCV(lasso, lasso_param_grid, scoring='r2', cv=5, n_jobs=-1)\n",
        "    lasso_cv.fit(X_train, y_train)\n",
        "    y_pred = lasso_cv.predict(X_valid)\n",
        "    print('Lasso best alpha:', lasso_cv.best_params_['model__alpha'])\n",
        "    print('Lasso valid R2:', r2_score(y_valid, y_pred))\n",
        "\n",
        "    # XGBoost\n",
        "    xgb = XGBRegressor(\n",
        "        n_estimators=500,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=4,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        objective='reg:squarederror',\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "    xgb.fit(X_train, y_train)\n",
        "    y_pred = xgb.predict(X_valid)\n",
        "    print('XGB valid R2:', r2_score(y_valid, y_pred))\n",
        "\n",
        "    # Choose best by valid R2\n",
        "    models = {\n",
        "        'ridge': ridge_cv.best_estimator_,\n",
        "        'lasso': lasso_cv.best_estimator_,\n",
        "        'xgb': xgb,\n",
        "    }\n",
        "    scores = {name: r2_score(y_valid, (m.predict(X_valid))) for name, m in models.items()}\n",
        "    best_name = max(scores, key=scores.get)\n",
        "    best_model = models[best_name]\n",
        "    print('Best model:', best_name, 'R2:', scores[best_name])\n",
        "\n",
        "    # Final test evaluation\n",
        "    y_pred_test = best_model.predict(X_test)\n",
        "    r2 = r2_score(y_test, y_pred_test)\n",
        "    mape = mean_absolute_percentage_error(y_test, y_pred_test)\n",
        "    print('Test R2:', r2)\n",
        "    print('Test MAPE:', mape)\n",
        "\n",
        "    # Save metrics\n",
        "    metrics = {'valid_scores': scores, 'test_r2': float(r2), 'test_mape': float(mape), 'best_model': best_name, 'features': numeric_cols}\n",
        "    (OUTPUTS_DIR / 'metrics.json').write_text(json.dumps(metrics, indent=2))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "0f42126a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plots: Parity and Residuals"
      ],
      "id": "5d4de854"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if df.empty or TARGET_COL not in df.columns:\n",
        "    pass\n",
        "else:\n",
        "    def parity_plot(y_true, y_pred, title, fname):\n",
        "        plt.figure(figsize=(5,5))\n",
        "        lims = [min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max())]\n",
        "        plt.scatter(y_true, y_pred, alpha=0.6)\n",
        "        plt.plot(lims, lims, 'r--')\n",
        "        plt.xlabel('True')\n",
        "        plt.ylabel('Predicted')\n",
        "        plt.title(title)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(OUTPUTS_DIR / fname, dpi=150)\n",
        "        plt.show()\n",
        "\n",
        "    # Using validation set predictions from best model\n",
        "    y_pred_valid = best_model.predict(X_valid)\n",
        "    parity_plot(y_valid, y_pred_valid, f'Parity plot ({best_name})', 'parity_valid.png')\n",
        "\n",
        "    residuals = y_valid - y_pred_valid\n",
        "    plt.figure(figsize=(6,4))\n",
        "    sns.histplot(residuals, kde=True)\n",
        "    plt.title('Residuals distribution (validation)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(OUTPUTS_DIR / 'residuals_valid.png', dpi=150)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "47808e81"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature importance (XGBoost)\n",
        "\n",
        "We report gain-based importances to inform policy-relevant drivers."
      ],
      "id": "01a7a974"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if df.empty or TARGET_COL not in df.columns:\n",
        "    pass\n",
        "else:\n",
        "    if isinstance(best_model, XGBRegressor):\n",
        "        importances = best_model.feature_importances_\n",
        "        fi = pd.DataFrame({'feature': X_train.columns, 'importance': importances}).sort_values('importance', ascending=False)\n",
        "        fi.head(20).to_csv(OUTPUTS_DIR / 'feature_importance_top20.csv', index=False)\n",
        "        plt.figure(figsize=(8,6))\n",
        "        sns.barplot(x='importance', y='feature', data=fi.head(20))\n",
        "        plt.title('Top 20 feature importances (XGB)')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(OUTPUTS_DIR / 'feature_importance_top20.png', dpi=150)\n",
        "        plt.show()\n",
        "    else:\n",
        "        print('Best model is not XGB; skipping importance plot.')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "c286aa66"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save model (optional)\n",
        "\n",
        "Persist the best model for later inference."
      ],
      "id": "5a57780d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pickle\n",
        "if not (df.empty or TARGET_COL not in df.columns):\n",
        "    with open(OUTPUTS_DIR / f'model_{best_name}.pkl', 'wb') as f:\n",
        "        pickle.dump(best_model, f)\n",
        "    print('Saved model to:', OUTPUTS_DIR / f'model_{best_name}.pkl')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "336f2c82"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}